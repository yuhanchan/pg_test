{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 4], x=[3, 1])\n",
      "['x', 'edge_index']\n",
      "tensor([[-1.],\n",
      "        [ 0.],\n",
      "        [ 1.]])\n",
      "tensor([[0, 1, 1, 2],\n",
      "        [1, 0, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# data.edge_index: Graph connectivity in COO format with shape [2, num_edges] and type torch.long\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "\n",
    "# data.x: Node feature matrix with shape [num_nodes, num_node_features]\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "# data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
    "# data.y: Target to train against (may have arbitrary shape), e.g., node-level targets of shape [num_nodes, *] or graph-level targets of shape [1, *]\n",
    "# data.pos: Node position matrix with shape [num_nodes, num_dimensions]\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data)\n",
    "print(data.keys)\n",
    "print(data['x'])\n",
    "print(data['edge_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "6\n",
      "21\n",
      "Batch(batch=[1002], edge_index=[2, 3774], x=[1002, 21], y=[32])\n",
      "Batch(batch=[1114], edge_index=[2, 4298], x=[1114, 21], y=[32])\n",
      "Batch(batch=[1271], edge_index=[2, 4454], x=[1271, 21], y=[32])\n",
      "Batch(batch=[890], edge_index=[2, 3396], x=[890, 21], y=[32])\n",
      "Batch(batch=[1089], edge_index=[2, 3766], x=[1089, 21], y=[32])\n",
      "Batch(batch=[1014], edge_index=[2, 3966], x=[1014, 21], y=[32])\n",
      "Batch(batch=[1101], edge_index=[2, 4248], x=[1101, 21], y=[32])\n",
      "Batch(batch=[1049], edge_index=[2, 4140], x=[1049, 21], y=[32])\n",
      "Batch(batch=[979], edge_index=[2, 3840], x=[979, 21], y=[32])\n",
      "Batch(batch=[1053], edge_index=[2, 3864], x=[1053, 21], y=[32])\n",
      "Batch(batch=[1053], edge_index=[2, 4266], x=[1053, 21], y=[32])\n",
      "Batch(batch=[1139], edge_index=[2, 4346], x=[1139, 21], y=[32])\n",
      "Batch(batch=[1030], edge_index=[2, 4084], x=[1030, 21], y=[32])\n",
      "Batch(batch=[1002], edge_index=[2, 3940], x=[1002, 21], y=[32])\n",
      "Batch(batch=[848], edge_index=[2, 3208], x=[848, 21], y=[32])\n",
      "Batch(batch=[1012], edge_index=[2, 3944], x=[1012, 21], y=[32])\n",
      "Batch(batch=[996], edge_index=[2, 3688], x=[996, 21], y=[32])\n",
      "Batch(batch=[1133], edge_index=[2, 4204], x=[1133, 21], y=[32])\n",
      "Batch(batch=[805], edge_index=[2, 3138], x=[805, 21], y=[24])\n"
     ]
    }
   ],
   "source": [
    "### Data Loading\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES', use_node_attr=True)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset.num_classes)\n",
    "print(dataset.num_node_features)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Cora', 'root': '/tmp/Cora', 'transform': None, 'pre_transform': None, 'pre_filter': None, '__indices__': None, 'data': Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708]), 'slices': {'x': tensor([   0, 2708]), 'edge_index': tensor([    0, 10556]), 'y': tensor([   0, 2708]), 'train_mask': tensor([   0, 2708]), 'val_mask': tensor([   0, 2708]), 'test_mask': tensor([   0, 2708])}, '__data_list__': [Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])], 'split': 'public'}\n",
      "{'x': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'edge_index': tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]]), 'edge_attr': None, 'y': tensor([3, 4, 4,  ..., 3, 3, 3]), 'pos': None, 'normal': None, 'face': None, 'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True])}\n",
      "{'x': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32), 'edge_index': tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
      "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]]), 'edge_attr': None, 'y': tensor([3, 4, 4,  ..., 3, 3, 3]), 'pos': None, 'normal': None, 'face': None, 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'train_mask': tensor([ True,  True,  True,  ..., False, False, False])}\n"
     ]
    }
   ],
   "source": [
    "### Example of GCNConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data.data import Data\n",
    "\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "dataset_quantized = Data(x=dataset[0].x.type(torch.int32), edge_index=dataset[0].edge_index, y=dataset[0].y, val_mask=dataset[0].val_mask, test_mask=dataset[0].test_mask, train_mask=dataset[0].train_mask)\n",
    "\n",
    "print(dataset.__dict__)\n",
    "print(dataset[0].__dict__)\n",
    "print(dataset_quantized.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7690\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-f5554b8e4280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_quantized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-f5554b8e4280>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 1433->16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGCNConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# 16->7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_node_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, requires_grad)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_subclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "\n",
    "### Non-quantized\n",
    "# Training\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16) # 1433->16\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)       # 16->7\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))\n",
    "\n",
    "### quantized\n",
    "# Training\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16) # 1433->16\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)       # 16->7\n",
    "        self.conv1.weight = Parameter(torch.Tensor(dataset.num_node_features, 16).type(torch.int32))\n",
    "        self.conv1.bias = Parameter(torch.Tensor(16, dtype=torch.int32))\n",
    "        self.conv2.weight = Parameter(torch.Tensor(16, dataset.num_classes, dtype=torch.int32))\n",
    "        self.conv2.bias = Parameter(torch.Tensor(dataset.num_classes, dtype=torch.int32))\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset_quantized.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting GCNConv\n"
     ]
    }
   ],
   "source": [
    "# Self implementing GCNConv using Message Passing\n",
    "try:# GCNConv is imported in previous cell, here we're defining our own\n",
    "    if GCNConv:\n",
    "        print(\"deleting GCNConv\")\n",
    "        del GCNConv \n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class myGCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(myGCNConv, self).__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = myGCNConv(dataset.num_node_features, 16) # 1433->16\n",
    "        self.conv2 = myGCNConv(16, dataset.num_classes)       # 16->7\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# Evaluation\n",
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / int(data.test_mask.sum())\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache, reduce\n",
    "import math\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def profile(module, display_cpu=True, display_gpu=True):\n",
    "    assert issubclass(module, torch.nn.Module)\n",
    "    monkey_patch_init(module)\n",
    "    return module\n",
    "\n",
    "def monkey_patch_init(_class):\n",
    "    old_init = _class.__init__\n",
    "    def new_init(self, *args, **kwargs):\n",
    "        old_init(self, *args, **kwargs)\n",
    "        self.profiler = Profiler(self)\n",
    "        _class.__str__ = self.profiler.__str__\n",
    "    _class.__init__ = new_init\n",
    "\n",
    "class Profiler(object):\n",
    "    def __init__(self, module):\n",
    "        \"\"\"\n",
    "        An operation is a graph node that performs computation on tensors.\n",
    "        \"\"\"\n",
    "        self._module = module\n",
    "        self._events = {\n",
    "            'forward': defaultdict(Event),\n",
    "            'backward': defaultdict(Event)}\n",
    "        self._operations = {}\n",
    "        self._enable = True\n",
    "\n",
    "        #consume generator\n",
    "        list(map(self._hook_operation, operations(self._module)))\n",
    "    \n",
    "    def _hook_operation(self, op):\n",
    "        def wrapper_call(op, *input, **kwargs):\n",
    "            # Wrapper function to \"__call__\", with time counter in it.\n",
    "            if not self._enable:\n",
    "                return self._operations[op.__class__](op, *input, **kwargs)\n",
    "\n",
    "            with torch.autograd.profiler.profile() as prof:\n",
    "                result = self._operations[op.__class__](op, *input, **kwargs)\n",
    "            \n",
    "            self._events['forward'][op] += Event(\n",
    "                cpu_time=int(prof.total_average().cpu_time),\n",
    "                gpu_time=int(prof.total_average().cuda_time),\n",
    "                parameters=count_elements(op.parameters()),\n",
    "                input_size=count_elements(input),\n",
    "                hits=1)\n",
    "            \n",
    "            def backward_pre_hook(*args):\n",
    "                if not self._enable:\n",
    "                    return\n",
    "                self._events['backward'][op].append(time.time())\n",
    "            #result.grad_fn.register_pre_hook(backward_pre_hook);\n",
    "            return result\n",
    "\n",
    "        # monky patch \"__call__\" with \"wrapper_call\"  for this operation`\n",
    "        if op.__class__ not in self._operations:\n",
    "            self._operations[op.__class__] = op.__class__.__call__\n",
    "            op.__class__.__call__ = wrapper_call\n",
    "\n",
    "        #def backward_post_hook(*args):\n",
    "        #    if not this_profiler.profiling_on:\n",
    "        #        return\n",
    "        #    # adds ending time\n",
    "        #    backward = this_profiler.record['backward']\n",
    "        #    backward[-1] = backward[-1] + (time.time(),) \n",
    "        #op.register_backward_hook(backward_post_hook)   \n",
    "    \n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_metrics(self, module):\n",
    "        if module in self._events['forward']:\n",
    "            #it's an operation\n",
    "            return self._events['forward'][module]\n",
    "        \n",
    "        return reduce(operator.add, map(self.get_metrics, module._modules.values()))\n",
    "    \n",
    "    def __str__(self, module=None, indentation=0, pre_msg=''):\n",
    "        tmpstr = ''\n",
    "        if module is None:\n",
    "            module = self._module\n",
    "            tmpstr += Event.header()\n",
    "\n",
    "        # this is an operation\n",
    "        metrics = self.get_metrics(module).tostring()\n",
    "    \n",
    "        if module.__class__ in self._operations:\n",
    "            return  tmpstr + metrics + indent(pre_msg + module.__repr__(), indentation) + '\\n'\n",
    "        \n",
    "        name = module.__class__.__name__\n",
    "        tmpstr += metrics + indent(pre_msg + name  + '(', indentation) + '\\n'\n",
    "        for key, sub_module in module._modules.items():\n",
    "            tmpstr +=  self.__str__(sub_module, indentation+2, pre_msg='(' + key + '): ')\n",
    "        tmpstr +=  indent(')',indentation+len(metrics)) + '\\n'\n",
    "        return tmpstr        \n",
    "\n",
    "class Event(object):\n",
    "    def __init__(self, cpu_time=0, gpu_time=0, parameters=0, input_size=0, hits=0):\n",
    "        self.cpu_time = cpu_time\n",
    "        self.gpu_time = gpu_time\n",
    "        self.parameters = parameters\n",
    "        self.input_size = input_size\n",
    "        self.hits = hits\n",
    "    \n",
    "    @classmethod\n",
    "    def header(cls):\n",
    "        header = format_columns(['CPU Time','GPU Time','Parameters','Input','Architecture'])\n",
    "        return '\\n'.join([header,'='*len(header),''])\n",
    "\n",
    "    def tostring(self):\n",
    "        return format_columns([\n",
    "                format_time(self.cpu_time),\n",
    "                format_time(self.gpu_time),\n",
    "                format_count(self.parameters),\n",
    "                format_count(self.input_size)])\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        return Event(\n",
    "            self.cpu_time + other.cpu_time,\n",
    "            self.gpu_time + other.gpu_time,\n",
    "            self.parameters + other.parameters,\n",
    "            self.input_size + other.input_size,\n",
    "            self.hits + other.hits)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "\n",
    "def format_columns(cols, width=10):\n",
    "    assert isinstance(cols, list)\n",
    "    return  ' ' + ' '.join(col.center(width,' ') for col in cols) + '  '\n",
    "\n",
    "def format_time(time_in_ns):\n",
    "    if not time_in_ns:\n",
    "        return '-'\n",
    "\n",
    "    human_powers = ['n','u','m','']\n",
    "    power = int(math.log(time_in_ns, 10) // 3)\n",
    "    return '{:.2f}{}s '.format(\n",
    "            time_in_ns/1000.**power,\n",
    "            human_powers[power])\n",
    "\n",
    "def format_count(n):\n",
    "    if not n:\n",
    "        return '-'\n",
    "\n",
    "    human_powers = ['','k','m','g']\n",
    "    power = int(math.log(n, 10) // 3)\n",
    "    return '{:.2f}{} '.format(\n",
    "            n/1000.**power,\n",
    "            human_powers[power])\n",
    "\n",
    "def operations(module):\n",
    "    \"\"\"\n",
    "    Given a module recursively transverse it\n",
    "    to find all atomic operations.\n",
    "\n",
    "    Atomic operations are the nodes in the graph which\n",
    "    perform computations on the tensors.\n",
    "    \"\"\"\n",
    "    if not len(list(module.children())):\n",
    "        # nn.Module who doesn't have sub nn.Module, hook it.\n",
    "        yield module\n",
    "\n",
    "    for name, sub_module in module.named_children():\n",
    "        if (isinstance(sub_module, torch.nn.Container)\n",
    "            or isinstance(sub_module, torch.nn.Sequential)\n",
    "            or isinstance(sub_module, torch.nn.ModuleList)\n",
    "            or isinstance(sub_module, torch.nn.Module)):\n",
    "            # Recursively visit their decendants.\n",
    "            for op in operations(sub_module): #python2 compatibility\n",
    "                yield op\n",
    "\n",
    "def indent(s, indent):\n",
    "    return '\\n'.join((indent* ' ') + line for line in s.split('\\n'))\n",
    "\n",
    "def count_elements(tensors):\n",
    "    return sum([reduce(operator.mul, t.size()) for t in tensors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: conv1.weight Density: 1.0\n",
      "Epoch 0: conv2.weight Density: 1.0\n",
      "Conv1 time: 0.0061147212982177734\n",
      "Relu time: 0.00012755393981933594\n",
      "Conv2 time: 0.0020704269409179688\n",
      "Time without SpMM: 0.01825571060180664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test SpMM\n",
    "# ! rm -rf Planetoid Planetoid_toTensor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# without SpMM\n",
    "dataset1 = Planetoid(\"Planetoid\", name=\"Cora\")\n",
    "data1 = dataset1[0]\n",
    "# print(data1)\n",
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset1.num_features, 16, cached=True) # 1433->16\n",
    "        self.conv2 = GCNConv(16, dataset1.num_classes, cached=True)  # 16->7\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        start = time.time()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        end = time.time()\n",
    "        print(\"Conv1 time:\", end-start)\n",
    "        start = time.time()\n",
    "        x = F.relu(x)\n",
    "        end = time.time()\n",
    "        print(\"Relu time:\", end-start)\n",
    "        start = time.time()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        end = time.time()\n",
    "        print(\"Conv2 time:\", end-start)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net1()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(data):\n",
    "#     with torch.autograd.profiler.profile() as prof:\n",
    "    model.train()\n",
    "#     print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "#     print(out)\n",
    "#     print(out.shape)X\n",
    "#     print(data.x)\n",
    "#     print(\"Density: \", np.count_nonzero(data.x.cpu().detach().numpy())/data.x.shape[0]/data.x.shape[1])\n",
    "#     print(\"Density: \", np.count_nonzero(out.cpu().detach().numpy())/out.shape[0]/out.shape[1])\n",
    "    loss = F.nll_loss(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "    \n",
    "start = time.time()\n",
    "for epoch in range(1):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            print(f\"Epoch {epoch}:\", name, \"Density:\", np.count_nonzero(param.data.cpu().detach().numpy())/param.data.shape[0]/param.data.shape[1])\n",
    "    loss = train(data1)\n",
    "#     print(data1.x.shape)\n",
    "#     print(data1.x)\n",
    "#     print(np.count_nonzero(data1.x))\n",
    "end = time.time()\n",
    "print(f\"Time without SpMM: {end-start}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------ #\n",
    "\n",
    "if False:\n",
    "    # with SpMM\n",
    "    dataset2 = Planetoid(\"Planetoid_toTensor\", name=\"Cora\", transform=T.ToSparseTensor())\n",
    "    data2 = dataset2[0]\n",
    "\n",
    "\n",
    "    class Net2(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net2, self).__init__()\n",
    "            self.conv1 = GCNConv(dataset2.num_features, 16, cached=True)\n",
    "            self.conv2 = GCNConv(16, dataset2.num_classes, cached=True)\n",
    "\n",
    "        def forward(self, x, adj_t):\n",
    "            x = self.conv1(x, adj_t)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x, adj_t)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    model = Net2()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    def train(data):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.adj_t)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return float(loss)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train(data2)\n",
    "    end = time.time()\n",
    "    print(f\"Time with SpMM: {end-start}\")\n",
    "    print(\"\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
