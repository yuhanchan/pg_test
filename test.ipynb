{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost of Normalization\n",
    "\n",
    "\n",
    "##### Senario 1: $(D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}}X)W$\n",
    "Do  $D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}}$ first, then do $(AX)W$, the number of operation would be:\n",
    "- $D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}}$: $2nnz$ Mult\n",
    "- [SpmDm] AX: $nnz*F_{in}$ Mult and $nnz*F_{in}$ Add\n",
    "- [DmDm] (AX)W: $N*F_{in}*F_{out}$ Mult and $N*F_{in}*F_{out}$ Add\n",
    "\n",
    "In total it's $2nnz + nnz*F_{in} + N*F_{in}*F_{out}$ Mult and $nnz*F_{in} + N*F_{in}*F_{out}$ Add\n",
    "\n",
    "##### Senario 2: $(D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}})(XW)$\n",
    "Do  $D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}}$ first, then do $A(XW)$, the number of operation would be:\n",
    "- $D^{\\frac{-1}{2}}AD^{\\frac{-1}{2}}$: $2nnz$ Mult\n",
    "- [DmDm] XW: $N*F_{in}*F_{out}$ Mult and $N*F_{in}*F_{out}$ Add\n",
    "- [SpmDm] A(XW): $nnz*F_{out}$ Mult and $nnz*F_{out}$ Add\n",
    "\n",
    "In total it's $2nnz + nnz*F_{out} + N*F_{in}*F_{out}$ Mult and $nnz*F_{out} + N*F_{in}*F_{out}$ Add\n",
    "\n",
    "##### Senario 3: $D^{\\frac{-1}{2}}(AD^{\\frac{-1}{2}}XW)$\n",
    "Do $AD^\\frac{-1}{2}$ only, and multiply $D^\\frac{-1}{2}$ after aggeregation:\n",
    "- $AD^\\frac{-1}{2}$: $nnz$ Mult\n",
    "- Aggr: $nnz*F_{in}$ Add\n",
    "- Comb: $N*F_{in}*F_{out}$ Mult and $N*F_{in}*F_{out}$ Add\n",
    "- Multiply $D^\\frac{-1}{2}$: $N*F_{out}$ Mult\n",
    "\n",
    "In total it's $nnz+N*F_{in}*F_{out}+N*F_{out}$ Mult and $nnz*F_{in} + N*F_{in}*F_{out}$ Add\n",
    "\n",
    "##### Senario 4: $D^{\\frac{-1}{2}}((AD^{\\frac{-1}{2}})(XW))$\n",
    "Do $AD^\\frac{-1}{2}$ only, and multiply $D^\\frac{-1}{2}$ after aggeregation:\n",
    "- $AD^\\frac{-1}{2}$: $nnz$ Mult\n",
    "- Aggr: $nnz*F_{in}$ Add\n",
    "- Comb: $N*F_{in}*F_{out}$ Mult and $N*F_{in}*F_{out}$ Add\n",
    "- Multiply $D^\\frac{-1}{2}$: $N*F_{out}$ Mult\n",
    "\n",
    "In total it's $nnz+N*F_{in}*F_{out}+N*F_{out}$ Mult and $nnz*F_{in} + N*F_{in}*F_{out}$ Add\n",
    "\n",
    "###### Example:\n",
    "dataset Cora: $N=2708$, $nnz=10556$, layer 1: $F_{in}=1433$, $F_{out}=16$\n",
    "\n",
    "S1: 77.25M Mult, 77.23M Add\n",
    "\n",
    "S2: 62.28M Mult, 62.26M Add\n",
    "\n",
    "S3: 62.14M Mult (80% of S1), 77.23M Add\n",
    "\n",
    "dataset Reddit: $N=233K$, $nnz=114.6M$, layer 1: $F_{in}=602$, $F_{out}=16$\n",
    "\n",
    "S1: 71.24B Mult, 71.24B Add\n",
    "\n",
    "S2:\n",
    "\n",
    "S3: 2.24B Mult (3.14% of S1), 71.24B Add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = \n",
      " [[1 0 0 1 0]\n",
      " [1 0 0 0 1]\n",
      " [1 0 1 0 1]\n",
      " [1 0 1 1 1]\n",
      " [1 0 0 1 1]]\n",
      "D = \n",
      " [[2 0 0 0 0]\n",
      " [0 2 0 0 0]\n",
      " [0 0 3 0 0]\n",
      " [0 0 0 4 0]\n",
      " [0 0 0 0 3]]\n",
      "\n",
      "DNorm = \n",
      " [[0.70710678 0.         0.         0.         0.        ]\n",
      " [0.         0.70710678 0.         0.         0.        ]\n",
      " [0.         0.         0.57735027 0.         0.        ]\n",
      " [0.         0.         0.         0.5        0.        ]\n",
      " [0.         0.         0.         0.         0.57735027]]\n",
      "ANorm1 = \n",
      " [[0.70710678 0.         0.         0.70710678 0.        ]\n",
      " [0.70710678 0.         0.         0.         0.70710678]\n",
      " [0.57735027 0.         0.57735027 0.         0.57735027]\n",
      " [0.5        0.         0.5        0.5        0.5       ]\n",
      " [0.57735027 0.         0.         0.57735027 0.57735027]]\n",
      "ANorm2 = \n",
      " [[0.5        0.         0.         0.35355339 0.        ]\n",
      " [0.5        0.         0.         0.         0.40824829]\n",
      " [0.40824829 0.         0.33333333 0.         0.33333333]\n",
      " [0.35355339 0.         0.28867513 0.25       0.28867513]\n",
      " [0.40824829 0.         0.         0.28867513 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import matrix_power\n",
    "import math\n",
    "\n",
    "A = np.random.choice(2, (5,5), p=[0.6, 0.4])\n",
    "D = np.zeros((5,5), dtype=int)\n",
    "DNorm = np.zeros((5,5), dtype=float)\n",
    "for i in range(D.shape[0]):\n",
    "    D[i][i] = sum(A[i])\n",
    "print(\"A = \\n\", A)\n",
    "print(\"D = \\n\", D)\n",
    "print(\"\")\n",
    "\n",
    "# Normalize\n",
    "for i in range(D.shape[0]):\n",
    "    if D[i][i]:\n",
    "        DNorm[i][i] = 1/math.sqrt(D[i][i])\n",
    "ANorm1 = np.matmul(DNorm, A)\n",
    "ANorm2 = np.matmul(ANorm1, DNorm)\n",
    "print(\"DNorm = \\n\", DNorm)\n",
    "print(\"ANorm1 = \\n\", ANorm1)\n",
    "print(\"ANorm2 = \\n\", ANorm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity of different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora Feature (torch.Size([2708, 1433])) Density:  0.012682692515830173\n",
      "CiteSeer Feature (torch.Size([3327, 3703])) Density:  0.008536202581826887\n",
      "PubMed Feature (torch.Size([19717, 500])) Density:  0.10022123041030583\n",
      "CoraFull Feature (torch.Size([19793, 8710])) Density:  0.006529172805355173\n",
      "DBLP Feature (torch.Size([17716, 1639])) Density:  0.0031750356895336373\n",
      "Yelp Feature (torch.Size([716847, 300])) Density:  0.999902350152822\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e00090c8706c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# print(reddit.num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IMDB Feature Density: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;31m# print(\"COLLAB Feature Density: \", get_density(collab.data.x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# print(reddit.__dict__)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e00090c8706c>\u001b[0m in \u001b[0;36mget_density\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cora Feature ({cora.data.x.shape}) Density: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_density\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.datasets import TUDataset # IMDB-BINARY, REDDIT-BINARY or PROTEINS\n",
    "from torch_geometric.datasets import Reddit # IMDB-BINARY, REDDIT-BINARY or PROTEINS\n",
    "from torch_geometric.datasets import Planetoid # Cora, CiteSeer and PubMed\n",
    "from torch_geometric.datasets import CitationFull # CoraFull, DBLP\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset # PATTERN, CLUSTER, MNIST, CIFAR10, TSP, CSL\n",
    "from torch_geometric.datasets import Yelp # Yelp\n",
    "from torch_geometric.datasets import Planetoid # Cora, CiteSeer and PubMed\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "cora      = Planetoid(root='/tmp/Cora', name='Cora')                   # Citation networks\n",
    "citeseer  = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')           # Citation networks\n",
    "pubmed    = Planetoid(root='/tmp/PubMed', name='PubMed')               # Citation networks\n",
    "corafull  = CitationFull(root='/tmp/CoraFull', name='cora')            # Citation networks\n",
    "dblp      = CitationFull(root='/tmp/DBLP', name='DBLP')                # Citation networks\n",
    "\n",
    "collab    = TUDataset(root='/tmp/COLLAB', name='COLLAB')               # Social networks\n",
    "imdb      = TUDataset(root='/tmp/IMDB-BINARY', name='IMDB-BINARY')     # Social networks\n",
    "# reddit    = TUDataset(root='/tmp/REDDIT-BINARY', name='REDDIT-BINARY') # Social networks\n",
    "reddit    = Reddit(root='/tmp/Reddit') # Social networks\n",
    "proteins  = TUDataset(root='/tmp/PROTEINS', name='PROTEINS')           # Bioinformatics\n",
    "\n",
    "pattern   = GNNBenchmarkDataset(root='/tmp/PATTERN', name='PATTERN')   # \n",
    "cluster   = GNNBenchmarkDataset(root='/tmp/CLUSTER', name='CLUSTER')   # \n",
    "mnist     = GNNBenchmarkDataset(root='/tmp/MNIST', name='MNIST')       # \n",
    "cifar10   = GNNBenchmarkDataset(root='/tmp/CIFAR10', name='CIFAR10')   # \n",
    "tsp       = GNNBenchmarkDataset(root='/tmp/TSP', name='TSP')           # \n",
    "csl       = GNNBenchmarkDataset(root='/tmp/CSL', name='CSL')           # \n",
    "\n",
    "yelp      = Yelp(root='/tmp/Yelp')                                     # \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_density(data):\n",
    "    return np.count_nonzero(data.cpu().detach().numpy())/data.shape[0]/data.shape[1]\n",
    "\n",
    "print(f\"Cora Feature ({cora.data.x.shape}) Density: \", get_density(cora.data.x))\n",
    "print(f\"CiteSeer Feature ({citeseer.data.x.shape}) Density: \", get_density(citeseer.data.x))\n",
    "print(f\"PubMed Feature ({pubmed.data.x.shape}) Density: \", get_density(pubmed.data.x))\n",
    "print(f\"CoraFull Feature ({corafull.data.x.shape}) Density: \", get_density(corafull.data.x))\n",
    "print(f\"DBLP Feature ({dblp.data.x.shape}) Density: \", get_density(dblp.data.x))\n",
    "print(f\"Yelp Feature ({yelp.data.x.shape}) Density: \", get_density(yelp.data.x))\n",
    "\n",
    "# print(f\"Proteins Feature ({proteins.data.x.shape}) Density: \", get_density(proteins.data.x))\n",
    "# print(f\"Pattern Feature ({pattern.data.x.shape}) Density: \", get_density(pattern.data.x))\n",
    "# print(f\"Pattern Feature ({cluster.data.x.shape}) Density: \", get_density(cluster.data.x))\n",
    "# print(f\"Pattern Feature ({mnist.data.x.shape}) Density: \", get_density(mnist.data.x))\n",
    "# print(f\"Pattern Feature ({cifar10.data.x.shape}) Density: \", get_density(cifar10.data.x))\n",
    "# print(f\"Pattern Feature ({tsp.data.x.shape}) Density: \", get_density(tsp.data.x))\n",
    "# print(f\"Pattern Feature ({csl.data.x.shape}) Density: \", get_density(csl.data.x))\n",
    "\n",
    "# print(yelp.__dict__)\n",
    "# print(dir(imdb))\n",
    "# print(reddit.num_node_features)\n",
    "# print(reddit.num_edge_features)\n",
    "# print(reddit.num_features)\n",
    "\n",
    "# print(\"IMDB Feature Density: \", get_density(imdb.data.x))\n",
    "# print(\"COLLAB Feature Density: \", get_density(collab.data.x))\n",
    "# print(reddit.__dict__)\n",
    "# print(\"Reddit Feature Density: \", get_density(reddit.data.x))\n",
    "# print(\"Proteins Feature Density: \", get_density(proteins.data.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora\n",
      "Conv1 time: 68.13454627990723 ms\n",
      "Relu time: 1.1546611785888672 ms\n",
      "Conv2 time: 15.990257263183594 ms\n",
      "Accuracy: 0.9090\n",
      "\n",
      "cora\n",
      "Conv1 time: 2049.2217540740967 ms\n",
      "Relu time: 2.872467041015625 ms\n",
      "Conv2 time: 1973.1969833374023 ms\n",
      "Accuracy: 0.5977\n",
      "\n",
      "CiteSeer\n",
      "Conv1 time: 84.1214656829834 ms\n",
      "Relu time: 1.5490055084228516 ms\n",
      "Conv2 time: 14.474868774414062 ms\n",
      "Accuracy: 0.8740\n",
      "\n",
      "PubMed\n",
      "Conv1 time: 288.62690925598145 ms\n",
      "Relu time: 3.010988235473633 ms\n",
      "Conv2 time: 48.21443557739258 ms\n",
      "Accuracy: 0.8050\n",
      "\n",
      "dblp\n",
      "Conv1 time: 505.56349754333496 ms\n",
      "Relu time: 2.6674270629882812 ms\n",
      "Conv2 time: 59.56864356994629 ms\n",
      "Accuracy: 0.8405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time each layer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "dataset = None\n",
    "data = None\n",
    "\n",
    "conv1_time = 0\n",
    "relu_time = 0\n",
    "conv2_time = 0\n",
    "\n",
    "class Net1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True) # 1433->16\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True)  # 16->7\n",
    "\n",
    "    def forward(self, data):\n",
    "        global conv1_time, relu_time, conv2_time\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        start = time.time()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        end = time.time()\n",
    "        conv1_time += (end-start)*1000\n",
    "        \n",
    "        start = time.time()\n",
    "        x = F.relu(x)\n",
    "        end = time.time()\n",
    "        relu_time += (end-start)*1000\n",
    "        \n",
    "        start = time.time()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        end = time.time()\n",
    "        conv2_time += (end-start)*1000\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "    \n",
    "for i in [cora, corafull, citeseer, pubmed, dblp]:\n",
    "    global dataset, data, model, optimizer\n",
    "    dataset = i\n",
    "    data = dataset[0]\n",
    "    model = Net1()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "#     pprint(dataset.__dict__)\n",
    "#     pprint(dataset)\n",
    "    print(dataset.name)\n",
    "#     print(data)\n",
    "    conv1_time = 0\n",
    "    relu_time = 0\n",
    "    conv2_time = 0\n",
    "    for epoch in range(20):\n",
    "        loss = train(data)\n",
    "    print(f\"Conv1 time: {conv1_time} ms\")\n",
    "    print(f\"Relu time: {relu_time} ms\")\n",
    "    print(f\"Conv2 time: {conv2_time} ms\")\n",
    "    model.eval()\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    if hasattr(data, \"test_mask\"):\n",
    "        correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        acc = correct / int(data.test_mask.sum())\n",
    "    else:\n",
    "        correct = int(pred.eq(data.y).sum().item())\n",
    "        acc = correct / int(data.y.shape[0])\n",
    "    print('Accuracy: {:.4f}'.format(acc))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.09511157388291475\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.09511157388291475\n",
      "Epoch  1\n",
      "Epoch  2\n",
      "Epoch  3\n",
      "Epoch  4\n",
      "Epoch  5\n",
      "Epoch  6\n",
      "Epoch  7\n",
      "Epoch  8\n",
      "Epoch  9\n",
      "Epoch  10\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.4510529055885679\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.4510529055885679\n",
      "Epoch  11\n",
      "Epoch  12\n",
      "Epoch  13\n",
      "Epoch  14\n",
      "Epoch  15\n",
      "Epoch  16\n",
      "Epoch  17\n",
      "Epoch  18\n",
      "Epoch  19\n",
      "Epoch  20\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.7291707807478951\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.7291707807478951\n",
      "Epoch  21\n",
      "Epoch  22\n",
      "Epoch  23\n",
      "Epoch  24\n",
      "Epoch  25\n",
      "Epoch  26\n",
      "Epoch  27\n",
      "Epoch  28\n",
      "Epoch  29\n",
      "Epoch  30\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.8397393318133674\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.8397393318133674\n",
      "Epoch  31\n",
      "Epoch  32\n",
      "Epoch  33\n",
      "Epoch  34\n",
      "Epoch  35\n",
      "Epoch  36\n",
      "Epoch  37\n",
      "Epoch  38\n",
      "Epoch  39\n",
      "Epoch  40\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.8863975010322603\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.8863975010322603\n",
      "Epoch  41\n",
      "Epoch  42\n",
      "Epoch  43\n",
      "Epoch  44\n",
      "Epoch  45\n",
      "Epoch  46\n",
      "Epoch  47\n",
      "Epoch  48\n",
      "Epoch  49\n",
      "Epoch  50\n",
      "torch.Size([2, 114615892])\n",
      "Accuracy w/o sample:  0.9112435595928406\n",
      "torch.Size([2, 5824125])\n",
      "Accuracy w/ sample:  0.9112435595928406\n",
      "Epoch  51\n"
     ]
    }
   ],
   "source": [
    "%%capture output\n",
    "import torch.nn.utils.prune as prune\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "dataset = None\n",
    "data = None\n",
    "\n",
    "conv1_time = 0\n",
    "relu_time = 0\n",
    "conv2_time = 0\n",
    "\n",
    "class Net2(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True) # 1433->16\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True)  # 16->7\n",
    "\n",
    "    def forward(self, data):\n",
    "        global conv1_time, relu_time, conv2_time\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        start = time.time()\n",
    "        x = self.conv1(x, edge_index)\n",
    "        end = time.time()\n",
    "        conv1_time += (end-start)*1000\n",
    "        \n",
    "        start = time.time()\n",
    "        x = F.relu(x)\n",
    "        end = time.time()\n",
    "        relu_time += (end-start)*1000\n",
    "        \n",
    "        start = time.time()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        end = time.time()\n",
    "        conv2_time += (end-start)*1000\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training\n",
    "model = None\n",
    "optimizer = None\n",
    "\n",
    "def train(data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "def eval_(data): # eval is reseved word\n",
    "    model.eval()\n",
    "    print(data.edge_index.shape)\n",
    "    _, pred = model(data).max(dim=1)\n",
    "    if hasattr(data, \"test_mask\"):\n",
    "        correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        acc = correct / int(data.test_mask.sum())\n",
    "    else:\n",
    "        correct = int(pred.eq(data.y).sum().item())\n",
    "        acc = correct / int(data.y.shape[0])\n",
    "    return acc\n",
    "\n",
    "def sample(edge_index, num): # edge sampling\n",
    "    edge_index = edge_index.detach().cpu().numpy()\n",
    "    with open('reddit_edge_index_split.pkl', 'rb') as f:\n",
    "        index_split = pickle.load(f)\n",
    "    index_split.insert(0, 0)\n",
    "    index_split.append(len(edge_index[0]))\n",
    "#     for i in range(1, len(edge_index[0])):\n",
    "#         if edge_index[0][i] != edge_index[0][i-1]:\n",
    "#             split_index.append(i)\n",
    "    ret = [[],[]]\n",
    "    for i in range(len(index_split)-1):\n",
    "        selection = list(np.random.choice(edge_index[1][index_split[i]:index_split[i+1]], num))\n",
    "        ret[0]+=[i]*num\n",
    "        ret[1]+=selection\n",
    "#     print(len(ret[0]))\n",
    "#     print(len(ret[1]))\n",
    "    ret = torch.LongTensor(ret)\n",
    "    return ret\n",
    "    \n",
    "# for i in [cora, corafull, citeseer, pubmed, dblp]:\n",
    "for i in [reddit]:\n",
    "    for _ in range(10):\n",
    "        global dataset, data, model, optimizer\n",
    "        dataset = i\n",
    "        data = dataset[0]\n",
    "        full_edge_index = data.edge_index\n",
    "        sample_edge_index = sample(full_edge_index, 25)\n",
    "#         data.edge_index = sample(full_edge_index, 25)\n",
    "        model = Net2()\n",
    "#         prune.l1_unstructured(model.conv1, name='weight', amount=0.9)\n",
    "#         prune.l1_unstructured(model.conv2, name='weight', amount=0.8)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "#         pprint(dataset.__dict__)\n",
    "#         pprint(data)\n",
    "#         print(dataset.name)\n",
    "#         print(data.edge_index)\n",
    "#         print(data.edge_index.shape)\n",
    "        conv1_time = 0\n",
    "        relu_time = 0\n",
    "        conv2_time = 0\n",
    "        for epoch in range(5000):\n",
    "            print(\"Epoch \", epoch)\n",
    "#             data.edge_index = sample(full_edge_index, 25) # resample every epoch\n",
    "            loss = train(data)\n",
    "            if not epoch%10: # evaluate every 10 epoch\n",
    "                acc = eval_(data)\n",
    "                print(\"Accuracy w/o sample: \", acc)\n",
    "                \n",
    "                data.edge_index = sample_edge_index\n",
    "                acc = eval_(data)\n",
    "                print(\"Accuracy w/ sample: \", acc)\n",
    "                data.edge_index = full_edge_index\n",
    "#                 if acc > 0.90:\n",
    "#                     print(f\"epoch {epoch}\")\n",
    "#                     break\n",
    "        print(f\"Conv1 time: {conv1_time} ms\")\n",
    "        print(f\"Relu time: {relu_time} ms\")\n",
    "        print(f\"Conv2 time: {conv2_time} ms\")\n",
    "\n",
    "\n",
    "        print('Accuracy: {:.4f}'.format(acc))\n",
    "        print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
